---
title: "Machine Learning Assignment"
author: "preethi"
date: "11/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description of the dataset

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the [website](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). 

The goal of this project is to determine the manner in which they did the exercise. This is the "Classe" Variable in the training set. 

What is the classe variable

Class A - exactly according to the specification
Class B - throwing the elbows to the front
Class C - lifting the dumbbell only halfway
Class D - lowering the dumbbell only halfway
Class E - throwing the hips to the front

We have already been given a training and test set.
First we lad the required the packages and the set the seed for uniformity

```{r library, results='hide'}
library(caret)
library(dplyr)
library(tidyr)
library(randomForest)
library(rpart)
library(rpart.plot)
set.seed(2)
```

Then we download the training and test file and read them appropriately (specifically the na's)

```{r load}
if (!file.exists("pml-training.csv")){    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile = "./pml-training.csv", quiet = TRUE)}

if (!file.exists("pml-testing.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile = "./pml-testing.csv", quiet = TRUE)}

training_data <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA","#DIV/0!",""))
test_data <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA","#DIV/0!",""))

dim(training_data)

dim(test_data)

```

We now have our training data.Looking at it, there are some columns that wont be used toward the prediction. we can remove those in both training and testing data.

```{r clean}

trains <- subset(training_data,select = -c(1:7))

test <- subset(test_data,select = -c(1:7))

dim(trains)

dim(test)

```

The training data is then partitioned furrther into a validation and training set.  

```{r partition}

trainindex <- createDataPartition (trains$classe, p = 0.6, list = FALSE)

pml_train <- trains[trainindex,]
pml_validate <- trains[-trainindex,]

dim(pml_train)
dim(pml_validate)
```

First we clean the training data set of columns that have too many Na's and then check to see if there are columns with near 0 variances. 

```{r clean}
training <- pml_train %>% 
  purrr::discard(~sum(is.na(.x))/length(.x)* 100 >= 90)

dim(training)

nzv <- nearZeroVar(training)

nzv
```
Since there are no near 0 variances after the dataset is cleared of na columns. 
We can go ahead and clear the validation and test sets the same way. 

```{r clean2}

Cleannames <- colnames(training)
Cleannames2 <- colnames(training[,-53])


pml_validate <- pml_validate[Cleannames]

test <- test[Cleannames2]

dim(pml_validate)

dim(test)

```

First we fit a desicion tree model with rpart and plot it. 

```{r rpart}
desc_tree <- rpart(classe~., data = training, method = "class")

prediction_desc <- predict(desc_tree, data = training, type = "class")

confusionMatrix(prediction_desc,training$classe)

rpart.plot(desc_tree)

```
Now we can use a Random Forest plot on the training data using Classe as the outcome and everything else as the predictor.

```{r caret}

forest_model <- randomForest(classe~.,data = training, na.action=na.omit, method = "class")
##predicting on the validation data set
prediction <- predict(forest_model,pml_validate, type = "class")
##confusion matrix with the same.
confusionMatrix(prediction, pml_validate$classe)

```

Conclusion 

The decision tree model had an accuracy of 0.75 and the random forest an accuracy of 0.99. 
We can also look at the predictors and there scores and the out of bag error rate  is about 0.007

```{r pred }
forest_model$importance

forest_model$err.rate[500,1]
```


